{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_llm():\n",
    "    return ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "def get_embeddings():\n",
    "    return OpenAIEmbeddings()\n",
    "\n",
    "embedding_function = get_embeddings()\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Elon Musk is founder of Tesla. He was born in South Africa and moved to the United States to pursue his entrepreneurial dreams.\",\n",
    "        metadata={\"source\": \"elon_musk.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Elon Musk is also the CEO and founder of SpaceX, a company dedicated to reducing space transportation costs and enabling the colonization of Mars.\",\n",
    "        metadata={\"source\": \"spacex.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Before Tesla and SpaceX, Elon Musk co-founded Zip2, an online city guide software for newspapers, which was sold for nearly $300 million.\",\n",
    "        metadata={\"source\": \"zip2.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Elon Musk is known for his ambitious goals, including the development of the Hyperloop, a high-speed transportation system, and Neuralink, a neurotechnology company.\",\n",
    "        metadata={\"source\": \"hyperloop_neuralink.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"In 2021, Elon Musk briefly became the world's richest person, reflecting the substantial impact of Tesla's stock performance and his business ventures.\",\n",
    "        metadata={\"source\": \"musk_richest_person.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Elon Musk's ventures include The Boring Company, which aims to reduce traffic congestion through a network of underground tunnels.\",\n",
    "        metadata={\"source\": \"boring_company.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Musk has also been involved in philanthropic efforts, including donations to education and health causes, and he has pledged to give away much of his wealth during his lifetime.\",\n",
    "        metadata={\"source\": \"musk_philanthropy.txt\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    grades: list[str]\n",
    "    llm_output: str\n",
    "    documents: list[str]\n",
    "    on_topic: bool\n",
    "    past_convo: str\n",
    "\n",
    "def retrieve_docs(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(input=question)\n",
    "    state[\"documents\"] = [doc.page_content for doc in documents]\n",
    "    return state\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    \"\"\"Boolean value to check whether a question is releated to the grave model person\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Question is about Elon Musk? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    past_convo = \"This is the past conversation: \" + str(state[\"past_convo\"])\n",
    "\n",
    "    system = \"\"\"You are a grader assessing the topic a user question. \\n\n",
    "        Only answer if the question is about one of the following topics:\n",
    "        1. Related to past conversations\n",
    "        2. Talk about any detail of Elon Musk\n",
    "\n",
    "        Examples: How will the weather be today -> No\n",
    "                  Is (User) still alive -> Yes\n",
    "                  What was (User) most humble moment -> Yes\n",
    "\n",
    "        If the question IS about these topics response with \"Yes\", otherwise respond with \"No\".\n",
    "        \"\"\"\n",
    "\n",
    "    system = system + past_convo\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = get_llm()\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({\"question\": question})\n",
    "    state[\"on_topic\"] = result.score\n",
    "    return state\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    on_topic = state[\"on_topic\"]\n",
    "    if on_topic.lower() == \"yes\":\n",
    "        return \"on_topic\"\n",
    "    return \"off_topic\"\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    llm = get_llm()\n",
    "    question = state[\"question\"]\n",
    "    past_convo = state[\"past_convo\"]\n",
    "    \n",
    "\n",
    "    template = \"\"\"Answer the question if only any detail related to past conversations.:\n",
    "    Past Conversation: {past_convo}\n",
    "    Question: {question}\n",
    "    Else reply \"I cant respond to that!\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"question\": question, \"past_convo\": past_convo})\n",
    "    state[\"llm_output\"] = result\n",
    "    return state\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Boolean values to check for relevance on retrieved documents.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'Yes' or 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def document_grader(state: AgentState):\n",
    "    docs = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "        Give a binary score 'Yes' or 'No' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = get_llm()\n",
    "    structured_llm = llm.with_structured_output(GradeDocuments)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        result = grader_llm.invoke({\"document\": doc, \"question\": question})\n",
    "        scores.append(result.score)\n",
    "    state[\"grades\"] = scores\n",
    "    return state\n",
    "\n",
    "def gen_router(state: AgentState):\n",
    "    grades = state[\"grades\"]\n",
    "\n",
    "    if any(grade.lower() == \"yes\" for grade in grades):\n",
    "        filtered_grades = [grade for grade in grades if grade.lower() == \"yes\"]\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"rewrite_query\"\n",
    "    \n",
    "def rewriter(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "        for retrieval. Look at the input and try to reason about the underlying semantic intent / meaning. If needed use past conversation.\"\"\"\n",
    "    past_convo = \"This is the past conversation: \" + str(state[\"past_convo\"])\n",
    "    system = system + past_convo\n",
    "\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    llm = get_llm()\n",
    "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    output = question_rewriter.invoke({\"question\": question})\n",
    "    state[\"question\"] = output\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't respond to that!\n",
      "Your name is Kgen.\n",
      "You are 21 years old.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(state: AgentState):\n",
    "    llm = get_llm()\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"documents\"]\n",
    "    past_convo = state[\"past_convo\"]\n",
    "    \n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context or past conversations.:\n",
    "    {context}\n",
    "    Past Conversation: {past_convo}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"question\": question, \"context\": context, \"past_convo\": past_convo})\n",
    "    state[\"llm_output\"] = result\n",
    "    return state\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"topic_decision\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve_docs\", retrieve_docs)\n",
    "workflow.add_node(\"rewrite_query\", rewriter)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"document_grader\", document_grader)\n",
    "\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.add_edge(\"retrieve_docs\", \"document_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"topic_decision\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"on_topic\": \"retrieve_docs\",\n",
    "        \"off_topic\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"document_grader\",\n",
    "    gen_router,\n",
    "    {\n",
    "        \"generate\": \"generate_answer\",\n",
    "        \"rewrite_query\": \"rewrite_query\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"rewrite_query\", \"retrieve_docs\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"topic_decision\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "past_convo = \"\"\"\n",
    "\n",
    "User: How is the weather?\n",
    "Bot: I can't respond to that!\n",
    "\n",
    "User: Who is the owner of bella vista?\n",
    "Bot: The owner of Bella Vista is Antonio Rossi.\n",
    "User: My name is Kgen.\n",
    "Bot: Nice, how are you Kgen?\n",
    "User: I am fine. Thanks for asking.\n",
    "Bot: Good.\n",
    "User: I am 21 years old studying in caldwell university.\n",
    "\"\"\"\n",
    "\n",
    "result = app.invoke({\"question\": \"How is the weather?\", \"past_convo\": past_convo})\n",
    "print(result[\"llm_output\"])\n",
    "\n",
    "result = app.invoke({\"question\": \"What is my name?\", \"past_convo\": past_convo})\n",
    "print(result[\"llm_output\"])\n",
    "\n",
    "result = app.invoke({\"question\": \"What is my age?\", \"past_convo\": past_convo})\n",
    "print(result[\"llm_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Kgen.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke({\"question\": \"What is my name\", \"past_convo\": past_convo})\n",
    "print(result[\"llm_output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
